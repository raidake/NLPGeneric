{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('../tasks/classification/models')\n",
    "sys.path.append('../tasks/classification/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "embeddings = word2vec_model.vectors\n",
    "w2v_vocab = word2vec_model.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98307, 35636)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vocab['UNK'], w2v_vocab['PAD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11380"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_vocab['pad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We ignore the words\n",
    "2. We map them to `UNK`, available in word2vec vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(vocab, text):\n",
    "    \"\"\"Tokenize a given text using NLTK, returning the corresponding in our pretrained embeddings\"\"\"\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize the text into words\n",
    "    token_ids = [self.vocab.get(token, self.vocab[\"UNK\"]) for token in tokens]  # Get token IDs\n",
    "    return {\"tokens\": tokens, \"ids\": token_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bach/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "class NLTKTokenizer:\n",
    "    def __init__(self, config=None):\n",
    "        self.vocab = {}\n",
    "        self.config = config or {}\n",
    "        self.pad_id = None  # Store pad_id for future use\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, vocab):\n",
    "        \"\"\"Load a tokenizer with a pre-built vocabulary from a saved file.\"\"\"\n",
    "        tokenizer = cls()\n",
    "        tokenizer.vocab = vocab\n",
    "        tokenizer.pad_id = vocab.get(\"PAD\", 1)  # Ensure pad_id is set\n",
    "        return tokenizer\n",
    "\n",
    "    def build_vocab(self):\n",
    "        \"\"\"Build vocabulary from the given dataset.\"\"\"\n",
    "        from collections import Counter\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        dataset = load_dataset(self.config[\"dataset\"])\n",
    "        train_dataset = dataset['train']\n",
    "        vocab = Counter(self.vocab)\n",
    "        for item in train_dataset:\n",
    "            tokens = word_tokenize(item['text'].lower())\n",
    "            vocab.update(tokens)\n",
    "        self.vocab = {word: idx for idx, (word, _) in enumerate(vocab.items(), 1)}  # Index starts at 1\n",
    "        #print(self.vocab)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize a given text using NLTK.\"\"\"\n",
    "        tokens = word_tokenize(text.lower())  # Tokenize the text into words\n",
    "        token_ids = [self.vocab.get(token, self.vocab[\"UNK\"]) for token in tokens]  # Get token IDs\n",
    "        # token_ids = [self.vocab.get(token, np.zeros_like(self.vocab['UNK'])) for token in tokens]  # Get token IDs\n",
    "\n",
    "        return {\"tokens\": tokens, \"ids\": token_ids}\n",
    "    \n",
    "    def save(self, folder_path):\n",
    "        \"\"\"Save the vocabulary to a file.\"\"\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        with open(os.path.join(folder_path, \"vocab.json\"), \"w\") as f:\n",
    "            json.dump(self.vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NLTKTokenizer.from_pretrained(w2v_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item[\"text\"]\n",
    "        label = item[\"label\"]\n",
    "        ids = self.tokenizer.tokenize(text)[\"ids\"]\n",
    "        length = len(ids)\n",
    "        ids = torch.tensor(ids)\n",
    "        return ids, length, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    training_bs,\n",
    "    val_bs,\n",
    "):\n",
    "    train_dataset = ClassificationDataset(dataset[\"train\"], tokenizer)\n",
    "    validation_dataset = ClassificationDataset(dataset[\"validation\"], tokenizer)\n",
    "    test_dataset = ClassificationDataset(dataset[\"test\"], tokenizer)\n",
    "    # partial function to be used in DataLoader\n",
    "    def padding_fn(batch):\n",
    "        # pad sequences in the same batch to be the same shape\n",
    "        (xx, lengths, yy) = zip(*batch)\n",
    "        xx_pad = pad_sequence(xx, batch_first=True, padding_value=tokenizer.pad_id)\n",
    "        return xx_pad, torch.tensor(lengths), torch.tensor(yy)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=training_bs, shuffle=True, collate_fn=padding_fn)\n",
    "    val_loader   = DataLoader(validation_dataset, batch_size=val_bs, shuffle=True, collate_fn=padding_fn)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=val_bs, shuffle=True, collate_fn=padding_fn)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    tokenizer=tokenizer, \n",
    "    dataset=dataset, \n",
    "    training_bs=32,\n",
    "    val_bs=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../tasks/classification/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output, direction=1):\n",
    "        super(RNNLayer, self).__init__()\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.i2h = nn.Linear(dim_input + dim_hidden, dim_hidden)\n",
    "        self.i2o = nn.Linear(dim_input + dim_hidden, dim_output)\n",
    "        self.direction = direction\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        outputs = []\n",
    "        if self.direction == 1:\n",
    "            for i in range(input.size()[1]):\n",
    "                combined = torch.cat((input[:, i, :], hidden), dim=1)\n",
    "                hidden = self.i2h(combined)\n",
    "                output_cell = self.i2o(combined)\n",
    "                outputs.append(output_cell)\n",
    "        else: \n",
    "            for i in range(input.size()[1]-1, -1, -1):\n",
    "                combined = torch.cat((input[:, i, :], hidden), dim=1)\n",
    "                hidden = self.i2h(combined)\n",
    "                output_cell = self.i2o(combined)\n",
    "                outputs.append(output_cell)\n",
    "        return torch.stack(outputs, dim=1) # (batch_size, seq_len, dim_output)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.dim_hidden)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, dim_input, dim_hidden, dim_output, pretrained_embeddings=None, freeze_embeddings=True):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is not None:\n",
    "            print(\"Loading pretrained word embeddings\")\n",
    "            self.token_embedding = nn.Embedding.from_pretrained(\n",
    "                torch.tensor(pretrained_embeddings, dtype=torch.float),\n",
    "                freeze=freeze_embeddings\n",
    "            )\n",
    "        else:\n",
    "            self.token_embedding = nn.Embedding(vocab_size, dim_input)\n",
    "\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_output = dim_output\n",
    "\n",
    "        self.rnn_layer = RNNLayer(dim_input, dim_hidden, dim_output)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def initialize(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        hidden = self.rnn_layer.init_hidden(input.size()[0])\n",
    "        embedded = self.token_embedding(input)\n",
    "        outputs = self.rnn_layer(embedded, hidden)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return Variable(torch.zeros((batch_size, self.dim_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(w2v_vocab)\n",
    "dim_input = embeddings[0].shape[0]\n",
    "dim_hidden = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained word embeddings\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vocab_size=len(w2v_vocab),\n",
    "            dim_input=dim_input,\n",
    "            dim_hidden=dim_hidden,  \n",
    "            dim_output=2,\n",
    "            pretrained_embeddings=embeddings,\n",
    "            freeze_embeddings=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pretrained word embeddings from Part 1 as inputs; do not update them during training (they are “frozen”).\n",
    "\n",
    "Design a simple recurrent neural network (RNN), taking the input word embeddings, and predicting a sentiment label for each sentence. To do that, you need to consider how to aggregate the word representations to represent a sentence.\n",
    "\n",
    "Use the validation set to gauge the performance of the model for each epoch during training. You are required to use accuracy  as the performance metric during validation and evaluation.\n",
    "\n",
    "Use the mini-batch strategy during training.  You may choose any preferred optimizer (e.g., SGD, Adagrad, Adam, RMSprop). Be careful when you choose your initial learning rate and mini-batch size. (You should use the validation set to determine the optimal configuration.) Train the model until the accuracy  score on the validation set is not increasing for a few epochs.\n",
    "\n",
    "Evaluate your trained model on the test dataset, observing the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = BinaryAccuracy()\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10000\n",
    "metric_log_interval = 2000\n",
    "eval_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntu_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
